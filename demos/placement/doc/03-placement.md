# Placement with ACM and Placement API

In this section we are going to see how we can manage the placement of APPs using the integration between Advance Cluster Management and Argo CD. Thanks to this integration when you import a new OpenShift Cluster in ACM, this cluster will be available in the Argo CD Controller and, using one of the `ApplicationSet` Generators that we reviewed (in this case the [Cluster Decision Resource Generator](https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Cluster-Decision-Resource/)), ACM can manage where to target the APP deployment.

In order to simplify the usage of the [Cluster Decision Resource Generator](https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Cluster-Decision-Resource/), ACM provides an API called [Placement API](https://open-cluster-management.io/concepts/placement/) that permits to design the desired placement behaviour by configuring a new `Placement` Object.


## Configure the environment

Since we are using a new API through an additional `Placement` manifest, we will need to create that object.

1. Access your OpenShift console in the **Hub cluster**.
2. Click the `+` button to add resources.
3. Paste the following content:

  ```yaml
  apiVersion: cluster.open-cluster-management.io/v1beta1
  kind: Placement
  metadata:
    annotations:
      argocd.argoproj.io/sync-wave: "3"
      argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    name: demo-placement
    namespace: openshift-gitops
  spec:
    clusterSets:
      - cloud
      - edge
    numberOfClusters: 1
    predicates:
      - requiredClusterSelector:
          claimSelector:
            matchExpressions:
              - key: demo.status
                operator: In
                values:
                  - good
                  - average
    prioritizerPolicy:
      configurations:
        - scoreCoordinate:
            builtIn: ResourceAllocatableCPU
          weight: 2
        - scoreCoordinate:
            builtIn: ResourceAllocatableMemory
          weight: 2
  ```

There are three main points in this decriptor:

* `clusterSets`: Where we indicate the "groups" of clusters configured in ACM that are part of the placement Scheduling

* `predicates`: Where we filter clusters out from those groups, in this case based on cluster metadata generated by a `ClusterClaim` object

* `prioritizerPolicy`: That we use to decide where we will deploy if the `numberOfClusters` is less than the number of available clusters.

So, based in the configuration above, We will select one single cluster (`numberOfClusters: 1`) from the clusters of the `clusterSets` "cloud" and "edge" that have the metadata `demo.status` equal to "good" or "average", and in case that there are more than just one, pick the one with more CPU and Memory available.

In this case, I simulated that an external system is including tags into the OpenShift cluster using a `ClusterClaim`, depending if it determinies that the cluster is in good state, or if it has some faults but it's ok to work or if it is in bad state. During the demo we will act as that external system changing manually those tags by modifying the `ClusterClaim` object.

  > **NOTE**
  >
  > This is an example. You have [more options](https://open-cluster-management.io/concepts/placement/) that can be configured, for example, you can select the cluster out from the `clustersets` simply by labeling the clusters that you want to use, or you can create subgroups of clusters and choose how many deployments of the APP you want per one of each group with the `decisionStrategy` section.

Two actions must be taken then, the first one is to assign the imported clusters into the `clusterSet` that we are going to use, and then we will need to assign the desired metadata to each cluster using the `ClusterClaim` object.

Let's start by assigning clusters to the `ClusterSets`:

1. Access the ACM console by selecting "All Clusters" in the top left on the **Hub cluster**.
2. Go to Infastructure > Clusters and click on the "Cluster Sets" tab
3. Select the "cloud" `clusterSet` and assign the `local-cluster` to it, then assign the edge-1 cluster to the "edge" `clusterSet`

Now since we will start by deploying on the cloud cluster, we will assign the value "good" to the `local-cluster` and "bad" to the `edge-1` by creating the corresponding `ClusterClaim` object in each cluster.

First in the `local-cluster`:

1. Access your OpenShift console in the **Hub cluster**.
2. Click the `+` button to add resources.
3. Paste the following content:

    ```yaml
    apiVersion: cluster.open-cluster-management.io/v1alpha1
    kind: ClusterClaim
    metadata:
      name: demo.status
    spec:
      value: good
    ```

Then in the `edge-1` cluster

1. Access your OpenShift console in the **edge-1** cluster.
2. Click the `+` button to add resources.
3. Paste the following content:

    ```yaml
    apiVersion: cluster.open-cluster-management.io/v1alpha1
    kind: ClusterClaim
    metadata:
      name: demo.status
    spec:
      value: bad
    ```

  > **NOTE**
  >
  > We are creating these objects at this moment. From now on, you don't create them again, you will need to modify the already existing manifest.

Now everyting should be ready. With this configuration only the `local-cluster` will be selected for deployment. You can check that this is true by opening the corresponding `PlacementDecision` object linked to the `Placement` that we created:


1. Access your OpenShift console in the **Hub cluster**.
2. Go to "Home > API Explorer" and look for "PlacementDecision" objects
3. Review all instances and open `demo-placement-decision-1`
4. Open the YAML view and at the end, in the `status` section you will see the allowed clusters for this `Placement` object, in this case only `local-cluster`.


## Deploy on Cloud

Ok, now that we have prepared the initial state of our clusters, we can deploy the application:

1. Access your OpenShift console in the **Hub cluster**.
2. Click the `+` button to add resources.
3. Paste the following content:

    ```yaml
    apiVersion: argoproj.io/v1alpha1
    kind: ApplicationSet
    metadata:
      labels:
        app.kubernetes.io/managed-by: demo-placement-global
      name: demo-placement-global
      namespace: openshift-gitops
    spec:
      generators:
        - matrix:
            generators:
              - git:
                  directories:
                    - path: demos/placement/apps/welcome/*
                  repoURL: 'https://github.com/luisarizmendi/openshift-edge-demos.git'
                  revision: main
              - clusterDecisionResource:
                  configMapRef: acm-placement
                  labelSelector:
                    matchLabels:
                      cluster.open-cluster-management.io/placement: demo-placement
                  requeueAfterSeconds: 180
      goTemplate: true
      goTemplateOptions:
        - missingkey=error
      template:
        metadata:
          labels:
            app.kubernetes.io/managed-by: demo-placement-global
          name: '{{.path.basename}}-{{.name}}'
        spec:
          destination:
            namespace: '{{.path.basename}}'
            server: '{{.server}}'
          project: demo-placement
          source:
            helm:
              valueFiles:
                - values.yaml
                - environment/values-{{.name}}.yaml
            path: '{{.path.path}}'
            repoURL: 'https://github.com/luisarizmendi/openshift-edge-demos.git'
            targetRevision: main
          syncPolicy:
            automated:
              prune: true
              selfHeal: true
    ```

The `ApplicationSet` object above is using two generators, like in the previous demo section. The first one is a "Git generator" to generate `Application` manifests based on directories in a Git repository. The second one is the `clusterDecisionResource` generator, that as commented before, generates an intermediate object (in this case is called `PlacementDecession`) that determines the target clusters.

It also selects the `Placement` created before by adding a maching label `cluster.open-cluster-management.io/placement: demo-placement` requirement.

You will see how the "hello" APP is deployed on the Cloud OpenShift (`local-cluster`).


## Deploy on Edge

Imagine that we, or any external system, decides that the deployment of the "Hello" APP must be done in the Edge clusters, not in the Cloud. 

In order to modify the placement behaviour we can patch the corresponding `ClusterClaim` objects with the new values. In this demo we will do it manually by opening the objects in the OpenShift console.

Let's start by the `edge-1` cluster

1. Access your OpenShift console in the **edge-1**.
2. Go to "Home > API Explorer" and look for "ClusterClaim" objects
3. Review all instances and open `demo.status`
4. Open the YAML view and change from "bad" to "good" the value.

Now two different things may happen:

### First case

You go to the Argo CD console and see how, after some seconds, the "hello" APP is also deployed on the `edge-1` cluster and, if you wait a little bit more, the APP will be deleted from the Cloud cluster, Why if we still didn't configure the `ClusterClaim`? Remember that we configured in the `Placement` manifest a `prioritizerPolicy` that will select the cluster with less CPU and memory used, and that we setup the maximum number of APP deployemnts to one using the `numberOfClusters` key.

  > **NOTE**
  >
  > Obviously, this will only happen if your edge cluster has more avialable resources than the hub cluster, what is something normal if you deployed two clusters with the same resources and you installed ACM and Argo CD in one of them

### Second case

If that didn't happen to you (so the `edge-1` cluster has less avialable CPU and memory than the Hub cluster) you can still remove the "hello" APP from the Hub cluster so it's installed in the `edge-1` cluster:

1. Access your OpenShift console in the **Hub Cluster**.
2. Go to "Home > API Explorer" and look for "ClusterClaim" objects
3. Review all instances and open `demo.status`
4. Open the YAML view and change from "good" to "bad" the value.

After some seconds, the "hello" APP will be deleted from the Cloud cluster and deployed on `edge-1`.


## Clean-Up

Once you have finished moving the app around your clusters, you can delete the `Application` and `ApplicationSet` objects:

1. Access your OpenShift console in the Hub cluster.
2. Click the `+` button to add resources.
3. Paste the following content:

    ```yaml
    apiVersion: batch/v1
    kind: Job
    metadata:
      generateName: cleanup-demo-placement-global-
      namespace: openshift-gitops
    spec:
      template:
        spec:
          serviceAccountName: openshift-gitops-argocd-application-controller
          containers:
            - name: delete-apps
              image: openshift/origin-cli:latest
              command: ["oc"]
              args: ["delete", "applications", "-n", "openshift-gitops", "-l", "app.kubernetes.io/managed-by=demo-placement-global"]
            - name: delete-appsets
              image: openshift/origin-cli:latest
              command: ["oc"]
              args: ["delete", "applicationsets", "-n", "openshift-gitops", "-l", "app.kubernetes.io/managed-by=demo-placement-global"]
          restartPolicy: Never
    ```

## Going beyond


play with dynamic assigment


latency


